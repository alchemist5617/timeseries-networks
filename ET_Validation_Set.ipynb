{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not import rpy package\n",
      "Could not import r-package RCIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathsys2/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import PCA_functions as pf\n",
    "import Extreme_functions as ef\n",
    "\n",
    "from tigramite import plotting as tp\n",
    "from tigramite.pcmci import PCMCI\n",
    "from tigramite.independence_tests import ParCorr, CMIknn\n",
    "import tigramite.data_processing as pp\n",
    "\n",
    "from Data import Data\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def deseasonalize(data,freq=12):\n",
    "    \"\"\"\n",
    "    The shape of data should be (time, index) \n",
    "    \"\"\"\n",
    "    n  = data.shape[1]\n",
    "    N  = data.shape[0]\n",
    "    data_deseasonal = np.zeros(data.shape)\n",
    "    for i in range(n):\n",
    "        temp = np.copy(data[:,i])\n",
    "        r = np.zeros((N))\n",
    "        for j in range(freq):\n",
    "            Idx = np.arange(j,N,freq)\n",
    "            if temp[Idx].std() == 0:\n",
    "                r[Idx] = 0\n",
    "            else:\n",
    "                r[Idx] = (temp[Idx] - temp[Idx].mean())/temp[Idx].std()\n",
    "        data_deseasonal[:,i] = np.copy(r)\n",
    "    return(data_deseasonal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def min_MSE_finder(count, result_sst, link,n_estimators=100, max_depth=5, tau=-1):\n",
    "    result =[]\n",
    "    link = link[link[:,1] <= tau]\n",
    "\n",
    "    df = pd.DataFrame({\"drought\":count, \"drought1\":count})\n",
    "    df.drought1 = df.drought1.shift(abs(tau))\n",
    "    df = df.dropna()\n",
    "    index = int(df.shape[0]*0.7)\n",
    "    dim = df.shape[1]\n",
    "\n",
    "    x_train, x_test = df.iloc[:index,1:dim], df.iloc[index:,1:dim] \n",
    "    y_train, y_test = df.iloc[:index,0], df.iloc[index:,0] \n",
    "    model = RandomForestRegressor(max_depth=max_depth, random_state=0, n_estimators=n_estimators)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    result.append(mean_squared_error(y_pred, y_test))\n",
    "\n",
    "\n",
    "    for z in range(len(link)):\n",
    "        df = pd.DataFrame({\"drought\":count, \"drought1\":count})\n",
    "        df.drought1 = df.drought1.shift(abs(tau))\n",
    "        for k in range(0,z+1):\n",
    "                if link[k,0] != 0:\n",
    "                    df[str(link[k,0]-1)] = result_sst[:,link[k,0]-1]\n",
    "                    df[str(link[k,0]-1)] = df[str(link[k,0]-1)].shift(abs(link[k,1]))\n",
    "                else:\n",
    "                    df[str(link[k,0])] = count\n",
    "                    df[str(link[k,0])] = df[str(link[k,0])].shift(abs(link[k,1]))\n",
    "        df = df.dropna()\n",
    "        index = int(df.shape[0]*0.7)\n",
    "        dim = df.shape[1]\n",
    "\n",
    "        x_train, x_test = df.iloc[:index,1:dim], df.iloc[index:,1:dim] \n",
    "        y_train, y_test = df.iloc[:index,0], df.iloc[index:,0] \n",
    "        model = RandomForestRegressor(max_depth=5, random_state=0, n_estimators=100)\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        result.append(mean_squared_error(y_pred, y_test))\n",
    "    return(result,link)\n",
    "\n",
    "\n",
    "def min_MSE_finder1(count, result_sst, link,n_estimators=100, max_depth=5, tau=-1):\n",
    "    result =[]\n",
    "    link = link[link[:,1] <= tau]\n",
    "    refined_link = []\n",
    "    df = pd.DataFrame({\"drought\":count, \"drought1\":count})\n",
    "    df.drought1 = df.drought1.shift(abs(tau))\n",
    "    df = df.dropna()\n",
    "    index = int(df.shape[0]*0.7)\n",
    "    dim = df.shape[1]\n",
    "\n",
    "    x_train, x_test = df.iloc[:index,1:dim], df.iloc[index:,1:dim] \n",
    "    y_train, y_test = df.iloc[:index,0], df.iloc[index:,0] \n",
    "    model = RandomForestRegressor(max_depth=max_depth, random_state=0, n_estimators=n_estimators)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    result.append(mean_squared_error(y_pred, y_test))\n",
    "\n",
    "\n",
    "    for z in range(len(link)):\n",
    "        df = pd.DataFrame({\"drought\":count, \"drought1\":count})\n",
    "        df.drought1 = df.drought1.shift(abs(tau))\n",
    "        for k in range(0,z+1):\n",
    "                if link[k,0] != 0:\n",
    "                    df[str(link[k,0]-1)] = result_sst[:,link[k,0]-1]\n",
    "                    df[str(link[k,0]-1)] = df[str(link[k,0]-1)].shift(abs(link[k,1]))\n",
    "                else:\n",
    "                    df[str(link[k,0])] = count\n",
    "                    df[str(link[k,0])] = df[str(link[k,0])].shift(abs(link[k,1]))\n",
    "        df = df.dropna()\n",
    "        index = int(df.shape[0]*0.7)\n",
    "        dim = df.shape[1]\n",
    "\n",
    "        x_train, x_test = df.iloc[:index,1:dim], df.iloc[index:,1:dim] \n",
    "        y_train, y_test = df.iloc[:index,0], df.iloc[index:,0] \n",
    "        model = RandomForestRegressor(max_depth=5, random_state=0, n_estimators=100)\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        mse = mean_squared_error(y_pred, y_test)\n",
    "        if mse < result[-1]:\n",
    "            result.append(mean_squared_error(y_pred, y_test))\n",
    "            refined_link.append(link[z])\n",
    "    return(result,np.array(refined_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ET_gamma = np.load(\"ET_gamma.npy\")\n",
    "N = ET_gamma.shape[0]\n",
    "n_nodes = ET_gamma.shape[1]\n",
    "extremes_treshold = -2\n",
    "count = []\n",
    "for i in range(N):\n",
    "    count.append(np.count_nonzero(ET_gamma[i,:] <= extremes_treshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = 12\n",
    "temporal_limits = {\"time_min\":datetime(1977, 1, 1, 0, 0),\"time_max\":datetime(2015, 12, 1, 0, 0) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = [\"../../nc/sst.mnmean.nc\"]\n",
    "code = [\"sst\"]\n",
    "missing = [-9.96921e+36]\n",
    "n_components = [30]\n",
    "\n",
    "df_cluster = []\n",
    "for j in range(len(name)):\n",
    "    d = Data('{}'.format(name[j]),code[j],temporal_limits,missing_value=missing[j])\n",
    "\n",
    "    result = d.get_data()\n",
    "    lon_list = d.get_lon_list()\n",
    "    lat_list = d.get_lat_list()\n",
    "    lon = d.get_lon()\n",
    "    lat = d.get_lat()\n",
    "\n",
    "    result = pf.deseasonalize(np.array(result))\n",
    "    weights = np.sqrt(np.abs(np.cos(np.array(lat_list)* math.pi/180)))\n",
    "    for i in range(len(weights)):\n",
    "        result[:,i] = weights[i] * result[:,i]\n",
    "\n",
    "    data = pd.DataFrame(result)\n",
    "\n",
    "    scale = StandardScaler()\n",
    "    scaled_data = scale.fit_transform(data)\n",
    "\n",
    "    pca = PCA(n_components=n_components[j])\n",
    "    pca_model = pca.fit(scaled_data)\n",
    "    df_cluster.append(pca_model.transform(scaled_data))\n",
    "    \n",
    "  #  loading_sst = pf.varimax(np.transpose(pca_model.components_), q=1000)\n",
    "  #  for z in range(loading_sst.shape[1]):\n",
    "   #     loading_sst[:,z] = loading_sst[:,z] / np.linalg.norm(loading_sst[:,z])\n",
    "        \n",
    "    loading_sst = np.transpose(pca_model.components_) \n",
    "    loading_sst = pd.DataFrame(loading_sst)\n",
    "\n",
    "    #df_cluster.append(np.matmul(np.array(data), np.array(loading_sst)))\n",
    "    #df_cluster.append(pca_model.transform(data))\n",
    "\n",
    "extremes_name  = [\"n_extremes\"]\n",
    "\n",
    "sst_name  = []\n",
    "for i in range(n_components[0]):\n",
    "    sst_name.append(\"SST_%d\"%i)\n",
    "\n",
    "var_names = extremes_name + sst_name\n",
    "\n",
    "result_extremes = np.array(count)\n",
    "result_extremes = result_extremes.reshape((len(count),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sst = np.array(df_cluster[0])\n",
    "\n",
    "\n",
    "result = np.concatenate((result_extremes,result_sst), axis=1)\n",
    "result = np.array(result)\n",
    "\n",
    "\n",
    "dataframe = pp.DataFrame(result,var_names=var_names)\n",
    "cond_ind_test = ParCorr()\n",
    "pcmci = PCMCI(dataframe=dataframe, cond_ind_test=cond_ind_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pcmci.run_pcmci(tau_max=4, pc_alpha=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pcmci.print_significant_links(p_matrix=results['p_matrix'],\n",
    "#                                     val_matrix=results['val_matrix'],\n",
    "#                                     alpha_level=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_matrix = pcmci.get_corrected_pvalues(p_matrix=results['p_matrix'], fdr_method='fdr_bh')\n",
    "#pcmci.print_significant_links(\n",
    "#        p_matrix = results['p_matrix'], \n",
    "#        q_matrix = q_matrix,\n",
    "#        val_matrix = results['val_matrix'],\n",
    "#        alpha_level = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_obj(\"results_ET_again\")\n",
    "q_matrix = np.load(\"q_matrix_ET_again.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_obj(\"results_st\")\n",
    "q_matrix = np.load(\"q_matrix_sst.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "    pq_matrix = results['p_matrix']\n",
    "    val_matrix = results['val_matrix']\n",
    "    alpha_level = 0.05\n",
    "    N = pq_matrix.shape[0]\n",
    "\n",
    "    link_dict = dict()\n",
    "    for j in range(N):\n",
    "        # Get the good links\n",
    "        good_links = np.argwhere(pq_matrix[:, j, 1:] <= alpha_level)\n",
    "        # Build a dictionary from these links to their values\n",
    "        links = {(i, -tau - 1): np.abs(val_matrix[i, j, abs(tau) + 1])\n",
    "                 for i, tau in good_links}\n",
    "        # Sort by value\n",
    "        link_dict[j] = sorted(links, key=links.get, reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = l+link_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "    result_dic = {}\n",
    "    for item in set(l):\n",
    "        result_dic[item] = l.count(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, -12): 3,\n",
       " (0, -11): 2,\n",
       " (0, -2): 3,\n",
       " (0, -1): 3,\n",
       " (1, -10): 2,\n",
       " (1, -4): 3,\n",
       " (2, -12): 3,\n",
       " (2, -11): 1,\n",
       " (2, -9): 1,\n",
       " (3, -12): 2,\n",
       " (4, -12): 1,\n",
       " (4, -8): 1,\n",
       " (5, -8): 2,\n",
       " (6, -5): 1,\n",
       " (6, -4): 1,\n",
       " (8, -12): 3,\n",
       " (10, -7): 2,\n",
       " (10, -6): 1,\n",
       " (11, -5): 1,\n",
       " (11, -4): 2,\n",
       " (14, -11): 1,\n",
       " (14, -6): 3,\n",
       " (14, -3): 2,\n",
       " (14, -2): 1,\n",
       " (17, -9): 1,\n",
       " (17, -7): 2,\n",
       " (18, -3): 2,\n",
       " (19, -9): 1,\n",
       " (19, -7): 1,\n",
       " (19, -1): 1,\n",
       " (21, -7): 1,\n",
       " (22, -12): 2,\n",
       " (22, -8): 3,\n",
       " (23, -5): 3,\n",
       " (24, -5): 1,\n",
       " (25, -2): 1,\n",
       " (25, -1): 2,\n",
       " (26, -9): 1,\n",
       " (29, -10): 3,\n",
       " (30, -10): 1,\n",
       " (31, -12): 1,\n",
       " (32, -5): 1,\n",
       " (32, -3): 1,\n",
       " (33, -8): 3,\n",
       " (34, -12): 1,\n",
       " (34, -11): 2,\n",
       " (34, -9): 1,\n",
       " (36, -2): 1,\n",
       " (38, -8): 1,\n",
       " (38, -2): 1,\n",
       " (39, -10): 2,\n",
       " (39, -9): 2,\n",
       " (40, -4): 2,\n",
       " (41, -1): 3,\n",
       " (43, -11): 2,\n",
       " (44, -5): 1,\n",
       " (45, -11): 1,\n",
       " (46, -11): 3,\n",
       " (46, -6): 1,\n",
       " (46, -3): 1,\n",
       " (48, -10): 2,\n",
       " (49, -6): 2,\n",
       " (50, -2): 2,\n",
       " (51, -11): 1,\n",
       " (51, -4): 1,\n",
       " (52, -10): 3,\n",
       " (52, -9): 1,\n",
       " (52, -7): 1,\n",
       " (52, -3): 2,\n",
       " (52, -2): 3,\n",
       " (54, -6): 2,\n",
       " (55, -2): 3,\n",
       " (56, -9): 1,\n",
       " (57, -6): 1,\n",
       " (58, -4): 3,\n",
       " (59, -5): 2,\n",
       " (59, -3): 1,\n",
       " (64, -4): 2,\n",
       " (65, -8): 2,\n",
       " (66, -1): 2,\n",
       " (67, -12): 1,\n",
       " (67, -9): 2,\n",
       " (67, -8): 1,\n",
       " (67, -6): 2,\n",
       " (67, -1): 1,\n",
       " (68, -7): 1,\n",
       " (68, -2): 2,\n",
       " (69, -11): 1,\n",
       " (69, -2): 2,\n",
       " (70, -6): 2,\n",
       " (70, -5): 1,\n",
       " (70, -1): 2,\n",
       " (71, -6): 1,\n",
       " (72, -11): 3,\n",
       " (72, -6): 1,\n",
       " (74, -8): 2,\n",
       " (75, -5): 1,\n",
       " (75, -4): 2,\n",
       " (75, -3): 1,\n",
       " (76, -7): 1,\n",
       " (76, -5): 1,\n",
       " (76, -2): 1}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"./result_dic_test_ET.txt\"\n",
    "with open(file_name, 'w') as f:\n",
    "    f.write(\"{}\\n\".format(i+1))\n",
    "    for key, values in result_dic.items():\n",
    "        string = \"{},{}\\n\".format(key,values)\n",
    "        f.write(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pcmci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tigramite.pcmci.PCMCI"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PCMCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
